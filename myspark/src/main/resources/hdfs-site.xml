<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
		<property>
			<name>dfs.nameservices</name>
			<value>mycluster</value>
		</property>

		<property>
			<name>dfs.ha.namenodes.mycluster</name>
			<value>nn1,nn2</value>
		</property>

		<property>
			<name>dfs.namenode.rpc-address.mycluster.nn1</name>
			<value>s101:8020</value>
		</property>
		<property>
			<name>dfs.namenode.rpc-address.mycluster.nn2</name>
			<value>s105:8020</value>
		</property>

		<property>
			<name>dfs.namenode.http-address.mycluster.nn1</name>
			<value>s101:50070</value>
		</property>
		<property>
			<name>dfs.namenode.http-address.mycluster.nn2</name>
			<value>s105:50070</value>
		</property>

		<property>
			<name>dfs.namenode.shared.edits.dir</name>
			<value>qjournal://s102:8485;s103:8485;s104:8485/mycluster</value>
		</property>

		<property>
			<name>dfs.client.failover.proxy.provider.mycluster</name>
			<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		</property>

		<property>
			<name>dfs.ha.fencing.methods</name>
			<value>
				sshfence
				shell(/bin/true)
			</value>
		</property>
		<property>
			<name>dfs.ha.fencing.ssh.private-key-files</name>
			<value>/home/centos/.ssh/id_rsa</value>
		</property>

	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		  <name>dfs.namenode.secondary.http-address</name>
		  <value>s105:50090</value>
	</property>
	<!-- NN的多目录配置 -->                                                               
	<property>                                                                            
		 <name>dfs.namenode.name.dir</name>                                                  
		 <value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
	</property>                                                                           
	<!-- DN的多目录配置 -->                                                               
	<property>                                                                            
		 <name>dfs.datanode.data.dir</name>                                                  
		 <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
	</property>                                                                           
	<property>                                          
		 <name>dfs.namenode.fs-limits.min-block-size</name>
		 <value>512</value>                                  
	</property>                                         
	<property>                                                 
		<name>dfs.hosts</name>                                 
		<value>/soft/hadoop/etc/hadoop/dfs_include.conf</value>
	</property>                                                

	<property>                                                 
		<name>dfs.hosts.exclude</name>                         
		<value>/soft/hadoop/etc/hadoop/dfs_exclude.conf</value>
	</property>                                                

	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.permissions.enabled</name>
		<value>false</value>
	</property>
</configuration>
